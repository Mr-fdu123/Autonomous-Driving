### 模仿学习
传统强化学习依赖奖励函数，但现实很难设计出一个好的奖励函数，比如planning工作。可以通过模仿学习的原理，学习人类驾驶的轨迹，来优化自动驾驶轨迹。模仿学习有三种：行为克隆、逆强化学习、生成对抗强化学习
#### 行为克隆(BC)
行为克隆就是让智能体学习专家的决策函数，记专家数据样本$(s_t,a_t)$，将$s_t$作为输入，$a_t$作为标签，要学习使得决策函数$\pi_\theta(s)$达到最优的参数$\theta^*$,即<br>
\[
\theta^{*} = \arg\min_{\theta} \; \mathbb{E}_{(s,a)\sim \mathcal{B}}
\left[ \mathcal{L}\big(\pi_{\theta}(s), a\big) \right]
\]
其中$\mathcal{B}$是转卡数据集，$\mathcal{L}$为loss函数
#### 生成对抗强化学习
##### 生成对抗网络(GAN)
生成器(generator)：作为“骗子”，从随机噪声中生成与数据样本相似的假样本<br>
判别器(discriminator)：作为“警察”，要判别真伪，判断是生成器的假样本还是真样本
生成器$G(z)$负责输入随机噪声$z$,输出生成分布$p(z)$,判别器负责预测输入的样本$z$是服从真实分布$p_{data}(z)$还是生成分布$p_g(z)$,也就是优化问题:<br>
\[
\min_G \max_D V(D,G)
= \mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] + \mathbb{E}_{z \sim p(z)}[\log (1 - D(G(z)))]
\]
模型期望达到的状态：<br>
$D(z)=1/2$<br>
$p_{data}(z)=p_g(z)$
生成对抗强化学习就是要使得智能体策略接近专家策略，将智能体学习到的分布作为生成器，判别器判断输入样本是否为智能体，接近1为智能体，接近0为专家数据<br>
记专家策略$\rho_\pi(s,a)$，智能体策略$\rho_E(s,a)$，判别器$D(\phi)$的目标loss函数为：
\[
\mathcal{L}(\phi)
= - \mathbb{E}_{\rho_{\pi}}[\log D_{\phi}(s,a)]- \mathbb{E}_{\rho_E}[\log(1 - D_{\phi}(s,a))]
\]
