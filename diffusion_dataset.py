import torch
import warnings
import joblib
from typing import Any, Dict, List, Tuple,Set
import logging
import pickle
import math
import os
from os.path import join
import gzip, json, pickle
import numpy as np
import shapely
from shapely.geometry import Polygon, MultiPolygon, LineString,MultiLineString,Point,MultiPoint,box
from shapely.ops import snap,linemerge
from shapely.strtree import STRtree
import cv2
from scipy import ndimage

logger = logging.getLogger(__name__)

#ç´¢å¼•ä¸º2~last_index-8
#é•¿åº¦ä¸ºlast_index-9
def filter_subsets(point_set_list: List[List[Tuple[float, float]]]) -> List[List[Tuple[float, float]]]:
    # 1. é¢„å¤„ç†ï¼šå°†åŸå§‹ç‚¹é›†è½¬æ¢ä¸º Shapely å¯¹è±¡å¹¶è®¡ç®—åŠ¨æ€é˜ˆå€¼
    curve_data: List[Dict[str, Any]] = [] # å¼•å…¥å­—å…¸æ•°ç»„ç®€åŒ–è¿‡ç¨‹
    
    for original_index, points in enumerate(point_set_list):
        if len(points) < 2:
            continue
        try:
            line = LineString(points)
            
            # --- åŠ¨æ€é˜ˆå€¼è®¡ç®—æ ¸å¿ƒé€»è¾‘ ---
            # è®¡ç®—ç›¸é‚»ç‚¹ä¹‹é—´çš„è·ç¦»
            pts_array = np.array(points) # points: [[1, 2], [1, 3]]
            # å‘é‡åŒ–è®¡ç®—ï¼šsqrt((x2-x1)^2 + (y2-y1)^2)
            segment_lengths = np.sqrt(np.sum(np.diff(pts_array, axis=0)**2, axis=1))
            avg_spacing = np.mean(segment_lengths)
            # è®¾ç½®é˜ˆå€¼ä¸ºå¹³å‡ç‚¹è·çš„ 3.5 å€ï¼ˆå– 3-4 å€çš„ä¸­ä½æ•°ï¼‰
            dynamic_tolerance = avg_spacing * 3.5
            # ---------------------------

            curve_data.append({
                'line': line,
                'points': points,
                'original_index': original_index,
                'tolerance': dynamic_tolerance
            })
        except Exception:
            continue

    N = len(curve_data)
    is_subset: List[bool] = [False] * N # è¦å»é™¤çš„

    for i in range(N):
        if is_subset[i]:
            continue
        
        C_i_data = curve_data[i] # Dict
        C_i_line = C_i_data['line']
        # ä½¿ç”¨å½“å‰è¢«æ£€æŸ¥æ›²çº¿ C_i çš„åŠ¨æ€é˜ˆå€¼
        current_tolerance = C_i_data['tolerance']
        
        # æå–ç«¯ç‚¹
        E_i_1 = Point(C_i_data['points'][0])
        E_i_2 = Point(C_i_data['points'][-1])

        for j in range(N):
            if i == j or is_subset[j]:
                continue 
            
            C_j_line = curve_data[j]['line']
            
            # 1. å¿«é€Ÿç«¯ç‚¹æ£€æŸ¥
            dist_e1 = E_i_1.distance(C_j_line)
            dist_e2 = E_i_2.distance(C_j_line)
            
            if dist_e1 > current_tolerance or dist_e2 > current_tolerance:
                continue

            # 2. é‡‡æ ·ç‚¹æ£€æŸ¥ï¼ˆHausdorff è·ç¦»çš„ç®€åŒ–ç‰ˆï¼‰
            max_dist_to_j = 0.0
            # æ­¥é•¿å¯ä»¥æ ¹æ®ç‚¹æ•°åŠ¨æ€è°ƒæ•´ï¼Œé‡‡æ · 1/10 çš„ç‚¹è¿›è¡Œç²¾ç»†æ£€æŸ¥
            check_points = C_i_line.coords[::10] 
            
            is_match = True
            for x, y in check_points:
                dist = Point(x, y).distance(C_j_line)
                if dist > current_tolerance:
                    is_match = False
                    break
            
            if is_match:
                is_subset[i] = True
                break

    # æ„é€ ç»“æœ
    result_list = [point_set_list[curve_data[i]['original_index']] 
                   for i in range(N) if not is_subset[i]]
    return result_list


def calculate_distance(p1, p2):
    """è®¡ç®—ä¸¤ç‚¹ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»."""
    p1 = np.array(p1)
    p2 = np.array(p2)
    return np.linalg.norm(p1 - p2)

# --- ä¸»ç®—æ³•å®ç° ---

def merge_segments_by_shortest_distance(segment_list):
    """
    å°†ä¸€ç»„æœ‰åºç‚¹é›†ï¼ˆçº¿æ®µï¼‰åˆå¹¶ä¸ºä¸€æ¡è¿ç»­çš„ LineStringã€‚
    ä½¿ç”¨è´ªå¿ƒç®—æ³•ï¼Œæ¯æ¬¡é€‰æ‹©ä¸å½“å‰ LineString ç«¯ç‚¹è·ç¦»æœ€è¿‘çš„æœªè¿æ¥çº¿æ®µè¿›è¡Œè¿æ¥ã€‚
    
    Args:
        segment_list (list[list[tuple]]): å¾…åˆå¹¶çš„ç‚¹é›†åˆ—è¡¨ï¼Œä¾‹å¦‚ [[(x1, y1), (x2, y2)], ...]ã€‚
        
    Returns:
        list[tuple]: åˆå¹¶åçš„ LineStringã€‚
    """
    if not segment_list:
        return []

    # 1. åˆå§‹åŒ–
    
    # è½¬æ¢ä¸ºå¯æ“ä½œçš„åˆ—è¡¨å‰¯æœ¬
    segments = [list(s) for s in segment_list] 
    
    # 1.1 é€‰æ‹©ç¬¬ä¸€ä¸ªçº¿æ®µä½œä¸ºåˆå§‹ LineString L
    # é€‰æ‹©æœ€é•¿çš„çº¿æ®µä½œä¸ºèµ·ç‚¹é€šå¸¸æ›´ç¨³å®š
    initial_segment = max(segments, key=len)
    segments.remove(initial_segment)
    L = initial_segment

    # 2. è¿­ä»£è¿æ¥ (å¾ªç¯ç›´åˆ°æ‰€æœ‰çº¿æ®µéƒ½è¢«åˆå¹¶)
    while segments:
        min_distance = float('inf')
        
        # å­˜å‚¨æœ€ä½³è¿æ¥ä¿¡æ¯: (segment_index, reverse_op, end_op)
        # reverse_op: æ˜¯å¦åè½¬çº¿æ®µ (True/False)
        # end_op: è¿æ¥åˆ° L çš„å“ªä¸€ç«¯ ('start'/'end')
        best_connection = None 
        
        # L çš„å››ä¸ªæ½œåœ¨è¿æ¥ç‚¹
        L_start = L[0]
        L_end = L[-1]
        # print(L_start,L_end)
        # éå†æ‰€æœ‰æœªè¿æ¥çš„çº¿æ®µ
        for i, S_candidate in enumerate(segments):
            S_start = S_candidate[0]
            S_end = S_candidate[-1]
            
            # è®¡ç®— 4 ç§è¿æ¥çš„å¯èƒ½æ€§åŠå…¶è·ç¦»
            
            # 1. L_end -> S_start (ä¸åè½¬ S_candidate, è¿æ¥åˆ° L å°¾éƒ¨)
            d1 = calculate_distance(L_end, S_start)
            if d1 < min_distance:
                min_distance = d1
                best_connection = (i, False, 'end') # (ç´¢å¼•, ä¸åè½¬, æ¥å°¾)

            # 2. L_end -> S_end (åè½¬ S_candidate, è¿æ¥åˆ° L å°¾éƒ¨)
            d2 = calculate_distance(L_end, S_end)
            if d2 < min_distance:
                min_distance = d2
                best_connection = (i, True, 'end') # (ç´¢å¼•, åè½¬, æ¥å°¾)

            # 3. L_start <- S_start (åè½¬ L, è¿æ¥åˆ° L å¤´éƒ¨)
            d3 = calculate_distance(L_start, S_start)
            if d3 < min_distance:
                min_distance = d3
                # (ç´¢å¼•, ä¸åè½¬ S_candidate, æ¥å¤´)
                # æ³¨æ„ï¼šL_startè¿æ¥S_startæ—¶ï¼ŒS_candidateé¡ºåºä¸å˜ï¼ŒLåè½¬
                best_connection = (i, True, 'start') 

            # 4. L_start <- S_end (ä¸åè½¬ L, è¿æ¥åˆ° L å¤´éƒ¨)
            d4 = calculate_distance(L_start, S_end)
            if d4 < min_distance:
                min_distance = d4
                # (ç´¢å¼•, åè½¬ S_candidate, æ¥å¤´)
                # æ³¨æ„ï¼šL_startè¿æ¥S_endæ—¶ï¼ŒS_candidateéœ€åè½¬
                best_connection = (i, False, 'start')

        # 3. æ‰§è¡Œæœ€ä½³è¿æ¥
        if best_connection is None:
            # ç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œé™¤é segments ä¸ºç©º (å¾ªç¯å·²é€€å‡º)
            break 
            
        segment_index, should_reverse, connect_to = best_connection
        
        # è·å–æœ€ä½³çº¿æ®µå¹¶ä»å¾…å¤„ç†åˆ—è¡¨ä¸­ç§»é™¤
        S_best = segments.pop(segment_index)
        
        # å‡†å¤‡ S_best: å¦‚æœéœ€è¦ï¼Œåè½¬
        if should_reverse:
            S_best.reverse()
        
        # print(S_best[0],S_best[-1])
        # åˆå¹¶åˆ° LineString L
        if connect_to == 'end':
            # print("end")
            # L = L + S_best
            L.extend(S_best) # å¢åŠ ä¸¤ä¸ªç‚¹
        elif connect_to == 'start':
            # print("start")
            # L = S_best + L
            L = S_best + L

    # print(L[0],L[-1])
    return LineString(L)


def _find_closest_centerline_point_and_direction_vector(centerline_points, boundary_point):
    C = np.array(centerline_points) # [[1, 3], [2, 3]]
    P_B = np.array(boundary_point)
    num_centerline = C.shape[0]
    if num_centerline < 2:
        return None, None, None 
    distances_sq = np.sum((C - P_B) ** 2, axis=1)
    i = np.argmin(distances_sq)
    C_closest = C[i]
    if i < num_centerline - 1:
        D = C[i+1] - C[i]
    else:
        D = C[i] - C[i-1]
    return C_closest, i, D

# åˆ¤æ–­è¾¹ç•Œçº¿åœ¨ä¸­å¿ƒçº¿å·¦ä¾§æˆ–å³ä¾§æˆ–åœ¨ä¸­å¿ƒçº¿ä¸Š
def _determine_single_segment_side(centerline_points, boundary_segment_points, tolerance=1e-6):
    boundary_points = np.array(boundary_segment_points)
    num_points = boundary_points.shape[0]
    if num_points == 1:
        indices_to_check = [0]
    elif num_points == 2:
        indices_to_check = [0, 1]
    else:
        indices_to_check = [0, num_points // 2, num_points - 1] 
    key_points = boundary_points[indices_to_check]
    side_scores = []
    for P_B in key_points:
        C_closest, i, D = _find_closest_centerline_point_and_direction_vector(centerline_points, P_B)
        if D is None:
            continue
        d_x, d_y = D
        N_right = np.array([d_y, -d_x])
        V = P_B - C_closest
        S = np.dot(V, N_right)
        side_scores.append(S)
    avg_score = np.mean(side_scores)
    if avg_score > tolerance:
        return 'Right'
    elif avg_score < -tolerance:
        return 'Left'
    else:
        return 'Ambiguous'


def classify_multiple_boundary_segments(centerline_geom, list_of_boundary_segments, tolerance=1e-6):
    centerline_points = np.array(centerline_geom.coords)
    if centerline_points.shape[0] < 2:
        return ['Error: Insufficient Centerline Points'] * len(list_of_boundary_segments)
    results = []
    for segment_points in list_of_boundary_segments:
        result = _determine_single_segment_side(centerline_points, segment_points, tolerance)
        results.append(result)
    return results

# ä¿®æ­£è½¦é“çº¿ï¼Œä¸èƒ½åå‘
def align_laneline_order(L: LineString, R: LineString) -> Tuple[LineString, LineString]:
    if not L.coords or not R.coords:
        print("è­¦å‘Š: è‡³å°‘ä¸€æ¡è½¦é“çº¿ä¸ºç©ºã€‚")
        return L, R
    L_start = Point(L.coords[0])
    L_end = Point(L.coords[-1])
    R_start = Point(R.coords[0])
    R_end = Point(R.coords[-1])
    S1 = LineString([L_start, R_start])
    S2 = LineString([L_end, R_end])
    are_intersecting = S1.intersects(S2)
    if are_intersecting:
        R_coords_reversed = list(R.coords)[::-1]
        R_aligned = LineString(R_coords_reversed)
        return L, R_aligned
    else:
        return L, R


def create_lane_polygon(left_boundary: LineString, right_boundary: LineString) -> Polygon:
    left_coords = list(left_boundary.coords)
    right_coords = list(right_boundary.coords)
    right_coords_reversed = right_coords[::-1] # ä»å°¾åˆ°å¤´è¯»å–æ•°åˆ—
    polygon_coords = left_coords + right_coords_reversed
    lane_polygon = Polygon(polygon_coords)
    if not lane_polygon.is_valid:
        lane_polygon=lane_polygon.buffer(0) # åˆæ³•æ£€æŸ¥
    return lane_polygon


def fast_intersection_query_keys(
    polygons_dict, # å‡è®¾ Any æ˜¯ Shapely Polygon/Geometry
    target_square,             # å‡è®¾ Any æ˜¯ Shapely Polygon/Geometry
    tree,
) -> List[Any]:
    if not polygons_dict:
        return []
    polygons = list(polygons_dict.values())
    keys = list(polygons_dict.keys())
    potential_indices = tree.query(target_square)
    intersecting_keys = []
    for index in potential_indices:
        try:
            index = int(index)
        except ValueError:
            continue
        poly = polygons[index]
        if poly.intersects(target_square):
            key = keys[index]
            intersecting_keys.append(key)
    return intersecting_keys

def interpolate_polyline(points: np.ndarray, t: int) -> np.ndarray:
    """copy from av2-api"""

    if points.ndim != 2:
        print("XXXX",points.shape)
        raise ValueError("Input array must be (N,2) or (N,3) in shape.")

    # the number of points on the curve itself
    n, _ = points.shape

    # equally spaced in arclength -- the number of points that will be uniformly interpolated
    eq_spaced_points = np.linspace(0, 1, t)

    # Compute the chordal arclength of each segment.
    # Compute differences between each x coord, to get the dx's
    # Do the same to get dy's. Then the hypotenuse length is computed as a norm.
    chordlen: np.ndarray = np.linalg.norm(np.diff(points, axis=0), axis=1)  # type: ignore
    # Normalize the arclengths to a unit total
    chordlen = chordlen / np.sum(chordlen)
    # cumulative arclength

    cumarc: np.ndarray = np.zeros(len(chordlen) + 1)
    cumarc[1:] = np.cumsum(chordlen)

    # which interval did each point fall in, in terms of eq_spaced_points? (bin index)
    tbins: np.ndarray = np.digitize(eq_spaced_points, bins=cumarc).astype(int)  # type: ignore

    # #catch any problems at the ends
    tbins[np.where((tbins <= 0) | (eq_spaced_points <= 0))] = 1  # type: ignore
    tbins[np.where((tbins >= n) | (eq_spaced_points >= 1))] = n - 1

    chordlen[tbins - 1] = np.where(
        chordlen[tbins - 1] == 0, chordlen[tbins - 1] + 1e-6, chordlen[tbins - 1]
    )

    s = np.divide((eq_spaced_points - cumarc[tbins - 1]), chordlen[tbins - 1])
    anchors = points[tbins - 1, :]
    # broadcast to scale each row of `points` by a different row of s
    offsets = (points[tbins, :] - points[tbins - 1, :]) * s.reshape(-1, 1)
    points_interp: np.ndarray = anchors + offsets

    return points_interp
# ç»™å‡ºç‚¹é›†ï¼Œæ‰¾åˆ°æ¯ä¸ªç‚¹çš„åŒ…å«å¤šè¾¹å½¢keysé›†åˆ
def get_keys_for_points_in_polygons(
    points_list,
    polygons_dict,
    tree,
) -> List[List[Any]]:
    if not polygons_dict or not points_list:
        return [[] for _ in points_list]

    # 1. å‡†å¤‡æ•°æ®å’Œç´¢å¼•
    polygons = list(polygons_dict.values())
    keys = list(polygons_dict.keys())
    results_keys: List[List[Any]] = []
    for point_xy in points_list:
        current_point = Point(point_xy)
        potential_indices = tree.query(current_point)
        point_keys: List[Any] = []
        for index in potential_indices:
            index = int(index)
            poly = polygons[index]
            if not poly.is_valid:
                poly=poly.buffer(0)
            if poly.contains(current_point):
                key = keys[index]
                point_keys.append(key)
        results_keys.append(point_keys)
    return results_keys
# coords:(N, 2) -> (N, 3)
def calculate_and_extend_with_heading_list(coords) -> np.ndarray:
    N = len(coords)
    if N < 2:
        if N == 1:
            return np.append(coords, [[0.0]], axis=1) 
        return np.array([]) 
    delta = np.diff(coords, axis=0)
    segment_headings = np.arctan2(delta[:, 1], delta[:, 0])
    point_headings = np.append(segment_headings, segment_headings[-1])
    heading_column = point_headings[:, np.newaxis] 
    extended_coords = np.hstack((coords, heading_column))
    return extended_coords

from shapely.geometry import Point
from typing import Dict, Any, List, Tuple
from shapely.strtree import STRtree
# å‡è®¾ LaneID, Polygon ç­‰ç±»å‹å·²åœ¨å…¶ä»–åœ°æ–¹å®šä¹‰

def get_lane_ids_from_trajectory(
    dic1,    # {LaneID: Polygon}
    dic2, # {LaneID: [NextLaneID]}
    trajectory, # è½¨è¿¹ç‚¹åºåˆ—
    lane_tree, # dic1.values()å»ºç«‹çš„
) -> List[Any]:
    
    # --- å…³é”®ä¿®æ”¹ï¼šæ•°æ®å‡†å¤‡ ---
    all_lanes = list(dic1.values())
    lane_ids = list(dic1.keys()) # è·å–æ‰€æœ‰è½¦é“ ID åˆ—è¡¨
    
    # ç§»é™¤ lane_poly_to_idï¼Œå› ä¸ºæˆ‘ä»¬å°†ç›´æ¥ä½¿ç”¨ lane_ids åˆ—è¡¨çš„ç´¢å¼•
        
    lane_ids_sequence = []
    last_lane_id = None
    lane_candidates = []
    # ä¸ºä¸€ä¸ªç‚¹æ‰¾åˆ°åŒ…å«çš„è½¦é“é¢ids
    def global_search(current_point: Point) -> List[Any]:
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ all_lanes å’Œ lane_ids åˆ—è¡¨çš„ç´¢å¼•
        if lane_tree:
            # STRtree.query() è¿”å›çš„æ˜¯ç´¢å¼•æ•°ç»„
            potential_indices = lane_tree.query(current_point)
            
            found_ids = []
            for index in potential_indices:
                # 1. é€šè¿‡ç´¢å¼•è·å–å¤šè¾¹å½¢
                poly = all_lanes[int(index)]
                
                # 2. ç²¾ç¡®æµ‹è¯•
                if poly.contains(current_point):
                    # 3. é€šè¿‡ç´¢å¼•è·å–å¯¹åº”çš„ LaneID
                    lane_id = lane_ids[int(index)]
                    found_ids.append(lane_id)
            
            return found_ids
        else:
            # é€€åŒ–ä¸ºæ…¢é€Ÿçº¿æ€§æœç´¢ (é€»è¾‘ä¸å˜)
            found_ids = [
                lane_id 
                for lane_id, poly in dic1.items() 
                if poly.contains(current_point)
            ]
            return found_ids
            
    for point_xy in trajectory:
        current_point = Point(point_xy)
        
        # 1. è·³è¿‡é‡å¤è½¦é“ (å¹³æ»‘å¤„ç†)
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¾èµ– dic1[last_lane_id] æ¥è·å–å¤šè¾¹å½¢ï¼Œå‡å®š last_lane_id ä»ç„¶æ˜¯ Key
        if last_lane_id and dic1[last_lane_id].contains(current_point):
            continue
            
        # 2. å±€éƒ¨æœç´¢ï¼šå›¾ç»“æ„é¢„æµ‹ä¸‹ä¸€å€™é€‰ (é€»è¾‘ä¸å˜)
        if last_lane_id and not lane_candidates:
            lane_candidates = dic2.get(last_lane_id, []) 
            if not lane_candidates:
                last_lane_id = None
                continue
                
        # 3. å±€éƒ¨ç­›é€‰ï¼šç¼©å°å€™é€‰èŒƒå›´ (é€»è¾‘ä¸å˜)
        filtered_candidates = []
        for lane_id in lane_candidates:
            if dic1[lane_id].contains(current_point):
                filtered_candidates.append(lane_id) 
        lane_candidates = filtered_candidates
        
        # 4. ç¡®è®¤ä¸å¤„ç† (é€»è¾‘ä¸å˜)
        if len(lane_candidates) == 1:
            current_lane_id = lane_candidates.pop()
            last_lane_id = current_lane_id
            lane_ids_sequence.append(current_lane_id)
            lane_candidates = []

        elif not lane_candidates:
            # æƒ…å†µ B: å±€éƒ¨æœç´¢å¤±è´¥ï¼Œæ‰§è¡Œå…¨å±€æœç´¢
            all_found_ids = global_search(current_point)
            
            if len(all_found_ids) == 1:
                current_lane_id = all_found_ids.pop()
                last_lane_id = current_lane_id
                lane_ids_sequence.append(current_lane_id)
            elif all_found_ids:
                lane_candidates = all_found_ids
                
        # 5. å¦‚æœ len(lane_candidates) > 1 (æ­§ä¹‰)ï¼Œåˆ™ä¿ç•™å€™é€‰é›†
        
    return lane_ids_sequence

class DiffusionDataset(torch.utils.data.Dataset):
    def __init__(self, log_file_path: str, map_file_path: str, cache_dir: str,
                 agent_num=32, lane_num=70, lane_len=20, route_num=25, static_objects_num=5):
        """
        :param log_file_path: é©¾é©¶ä¿¡æ¯æ–‡ä»¶ï¼ˆå¦‚æ—¥å¿—æˆ–ä¼ æ„Ÿå™¨æ•°æ®ï¼‰çš„è·¯å¾„ã€‚
        :param map_file_path: åœ°å›¾æ–‡ä»¶ï¼ˆå¦‚ HD Map æ•°æ®ï¼‰çš„è·¯å¾„ã€‚
        """

        self.log_file_path = log_file_path
        self.map_file_path = map_file_path
        self._cache_dir=cache_dir
        self._log_file=[]
        self.load_file()
        self.agent_num = agent_num
        self.lane_num = lane_num
        self.lane_len = lane_len
        self.route_num = route_num
        self.static_objects_num = static_objects_num
        self.lane_info={} # [center_ls,lbound_ls,rbound_ls], åä¸¤ä¸ªåŒ…å›´æˆè½¦é“é¢
        self.crs_info={}  # Polygon(crs_points)
        self.next_lane={} # [(r_id, l_id), ]
        #self.tls={} # {(r_id, l_id): [0, 0, 1, 0]}
        self.trajectory=[]
        self.route_ids=None
        self.get_trajectory()
        self.process_map()
        self.lane_polygons={key:create_lane_polygon(lane[1],lane[2]) for key,lane in self.lane_info.items()}
        self.lane_tree=STRtree(list(self.lane_polygons.values()))
        self.crs_tree=STRtree(list(self.crs_info.values()))
        self.route_ids=get_lane_ids_from_trajectory(self.lane_polygons,self.next_lane,self.trajectory,self.lane_tree)
        self.radius=120.0
        # äº¤é€šç¯ä¿¡æ¯
        self.tls_dict = {0:[0, 0, 1, 0], 1:[1, 0, 0, 0], 2:[0, 1, 0, 0], 3:[0, 0, 0, 1]}

    @staticmethod
    def _find_ego_bbox(anno: Dict) -> Dict:
        for box in anno.get('bounding_boxes', []):
            if box.get('class') == 'ego_vehicle':
                return box
        return {}

    @staticmethod
    def _to_right_hand_transform(w2e: np.ndarray) -> np.ndarray:
        # Convert left-hand CARLA world2ego to right-hand coordinates (flip y).
        flip = np.diag([1.0, -1.0, 1.0, 1.0]).astype(w2e.dtype)
        return flip @ w2e @ flip

    def load_file(self):
        for ann_name in sorted(os.listdir(join(self.log_file_path,'anno')),key= lambda x: int(x.split('.')[0])):
            self._log_file.append(join(self.log_file_path,'anno',ann_name))
    
    def __len__(self) -> int:
        return len(self._log_file)

    def __getitem__(self, idx: int) -> Dict:
       data = self.build_features(idx)
       return data

    
    def build_features(self, idx: int) -> Dict:
        cache_path = None
        if self._cache_dir:
            folder_name = os.path.basename(self.log_file_path)
            # ä»…ä¿®æ”¹åç¼€åä»¥ä¾¿è¯†åˆ«æ–°æ ¼å¼
            cache_filename = f"{folder_name}_idx{idx}.joblib"
            
            full_cache_dir = os.path.join(self._cache_dir, folder_name)
            if not os.path.exists(full_cache_dir):
                os.makedirs(full_cache_dir, exist_ok=True)
            
            cache_path = os.path.join(full_cache_dir, cache_filename)
        
        # 2. æ£€æŸ¥ç¼“å­˜
        if cache_path and os.path.exists(cache_path):
            try:
                # å‘½ä¸­ç¼“å­˜ï¼šä½¿ç”¨ joblib åŠ è½½ï¼Œå®ƒä¼šè‡ªåŠ¨æ¢å¤åŸå§‹çš„ numpy ç»“æ„å’Œ dtype
                data = joblib.load(cache_path)
                return data
            except Exception as e:
                logger.warning(f"Failed to load cache from {cache_path}: {e}. Recalculating features.")

        with gzip.open(self._log_file[idx], 'rt', encoding='utf-8') as gz_file:
            anno = json.load(gz_file)
        
        # diffusion plannerç”¨çš„è‡ªè½¦åæ ‡ç³»ï¼Œå¾—åˆ°è½¬åŒ–çŸ©é˜µ
        ego_box = self._find_ego_bbox(anno)
        if not ego_box:
            raise ValueError(f"ego_vehicle not found in {self._log_file[idx]}")
        world2ego = np.array(ego_box['world2ego'], dtype=np.float32)
        world2ego = self._to_right_hand_transform(world2ego)
        rot = world2ego[:3, :3]

        
        data = {}
        speed = float(anno.get('speed', 0.0))
        yaw_lh = float(anno.get('theta', 0.0))
        yaw_rh = -yaw_lh
        v_world = np.array([speed * math.cos(yaw_rh), speed * math.sin(yaw_rh), 0.0], dtype=np.float32)
        v_ego = rot @ v_world

        accel = anno.get('acceleration', [0.0, 0.0, 0.0])
        a_world = np.array([accel[0], -accel[1], accel[2]], dtype=np.float32)
        a_ego = rot @ a_world

        steer = float(anno.get('steer', 0.0))
        yaw_rate = float(anno.get('angular_velocity', [0.0, 0.0, 0.0])[2])
        yaw_rate = -yaw_rate

        data['ego_current_state'] = np.array([0.0, 0.0, 1.0, 0.0,
                                       v_ego[0], v_ego[1], a_ego[0], a_ego[1],
                                       steer, yaw_rate], dtype=np.float32)
        ego_agent_future, agent_features_past, agent_features_future = self.get_agent_features(idx)
        data['ego_agent_future'] = ego_agent_future
        data['neighbor_agents_past'] = agent_features_past
        data['neighbor_agents_future'] = agent_features_future
        data['static_objects'] = self.get_static()
        ego_center = ego_box.get('center') or ego_box.get('location')
        q_xy = (ego_center[0], -ego_center[1])
        lanes, lanes_speed_limit, lanes_has_speed_limit, route_lanes, route_lanes_speed_limit, route_lanes_has_speed_limit=self.get_map_features(idx, q_xy)
        data['lanes'] = lanes
        data['lanes_speed_limit'] = lanes_speed_limit
        data['lanes_has_speed_limit'] = lanes_has_speed_limit
        data['route_lanes'] = route_lanes
        data['route_lanes_speed_limit'] = route_lanes_speed_limit
        data['route_lanes_has_speed_limit'] = route_lanes_has_speed_limit

        # 3. å­˜å‚¨ç¼“å­˜
        if cache_path:
            try:
                # ä»…ä¿®æ”¹å­˜å‚¨æ–¹å¼ï¼šä½¿ç”¨ joblib å¹¶å¯ç”¨ zstd å‹ç¼©
                # è¿™ä¸ä¼šæ”¹å˜æ•°æ®å†…å®¹ï¼Œåªä¼šæ”¹å˜ç£ç›˜ä¸Šçš„äºŒè¿›åˆ¶æ’åˆ—æ ¼å¼
                joblib.dump(data, cache_path, compress='lz4')
                logger.info(f"Cache saved to {cache_path}")
            except Exception as e:
                logger.error(f"Failed to save cache for {cache_path}: {e}")

        # final_data = PlutoFeature.normalize(data, first_time=True, radius=100.0)
        return data

    def get_trajectory(self) -> None:
        cache_path = None
        if self._cache_dir and self._log_file:
            folder_name = os.path.basename(self.log_file_path)
            full_cache_dir = os.path.join(self._cache_dir, "AAtrajectory_cache")
            if not os.path.exists(full_cache_dir):
                os.makedirs(full_cache_dir)
            cache_path = os.path.join(full_cache_dir, f"{folder_name}_trajectory.pkl")
        if cache_path and os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    self.trajectory = pickle.load(f)
                logger.info(f"âœ… Trajectory loaded from cache: {cache_path}")
                return # ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥è¿”å›
            except Exception as e:
                logger.warning(f"Failed to load trajectory cache from {cache_path}: {e}. Recalculating trajectory.")
        # logger.info(f"â³ Processing trajectory from {len(self._log_file)} log files...")
        self.trajectory = [] 
        for file_path in self._log_file:
            with gzip.open(file_path, 'rt', encoding='utf-8') as gz_file:
                anno = json.load(gz_file)
                ego_box = self._find_ego_bbox(anno)
                if not ego_box:
                    continue
                ego_loc = ego_box.get('location') or ego_box.get('center')
                self.trajectory.append((ego_loc[0], -ego_loc[1]))
        if cache_path:
            try:
                with open(cache_path, 'wb') as f:
                    pickle.dump(self.trajectory, f)
                logger.info(f"ğŸ’¾ Trajectory successfully cached to {cache_path}")
            except Exception as e:
                logger.error(f"Failed to save trajectory cache for {cache_path}: {e}")

    def get_agent_features(self,idx):
        T=101
        N = self.agent_num
        ego_agent_future = np.zeros((80, 3), dtype=np.float32)
        agent_features_past = np.zeros((N, 21, 11), dtype=np.float32)
        agent_features_future = np.zeros((N, 80, 3), dtype=np.float32)
        agent_list=[] # å­˜å‚¨agent dict(no traffic)
        id_list=[]
        id_dict={}
        ego_id=None
        w2e = None
        rot = None
        with gzip.open(self._log_file[idx], 'rt', encoding='utf-8') as gz_file:
            anno = json.load(gz_file)
            for dic in anno['bounding_boxes']:
                if dic['class'] == 'ego_vehicle':
                    ego_id = dic['id']
                    # å¾—åˆ°ä¸–ç•Œ->è‡ªè½¦åæ ‡ç³»è½¬åŒ–çŸ©é˜µ
                    world2ego = dic['world2ego']
                    w2e = np.array(world2ego, dtype=np.float32)
                    w2e = self._to_right_hand_transform(w2e)
                    rot = w2e[:3, :3]
                    continue
                if dic['class']=='traffic_light':continue
                if dic['class']=='traffic_sign':continue
                id = dic['id']
                if id not in id_list:
                    id_list.append(id)
                    agent_list.append(dic)
        if w2e is None:
            raise ValueError(f"ego_vehicle not found in {self._log_file[idx]}")
        agent_list=sorted(agent_list, key=lambda x: x['distance'])[:N]
        for i,agent in enumerate(agent_list):
            id_dict[agent['id']]=i
        st=max(0,idx-20)
        ed=min(len(self._log_file),idx+81)
  
        for t in range(st, ed):
            with gzip.open(self._log_file[t], 'rt', encoding='utf-8') as gz_file:
                anno = json.load(gz_file)
                for dic in anno['bounding_boxes']:
                    id = dic['id']
                    if (id == ego_id):
                        tt = t-idx+20
                        p_w = np.array([dic['location'][0], -dic['location'][1], dic['location'][2], 1.0], dtype=np.float32)
                        p_ego = w2e @ p_w.T
                        x = p_ego[0]
                        y = p_ego[1]
                        heading_w = -math.radians(dic['rotation'][2])
                        h_w = np.array([np.cos(heading_w), np.sin(heading_w), 0.0], dtype=np.float32)
                        h_ego = rot @ h_w
                        heading_ego = np.arctan2(h_ego[1], h_ego[0])
                        future_idx = t - idx - 1
                        if 0 <= future_idx < 80:
                            ego_agent_future[future_idx] = np.array([x, y, heading_ego], dtype=np.float32)
                        continue
                    if (id not in id_list):
                        continue
                    rid=id_dict[id]
                    tt = t-idx+20
                    p_w = np.array([dic['location'][0], -dic['location'][1], dic['location'][2], 1.0], dtype=np.float32)
                    p_ego = w2e @ p_w.T
                    x = p_ego[0]
                    y = p_ego[1]
                    speed = float(dic.get('speed', 0.0))
                    yaw_rh = -math.radians(dic['rotation'][2])
                    h_w = np.array([np.cos(yaw_rh), np.sin(yaw_rh), 0.0], dtype=np.float32)
                    h_ego = rot @ h_w
                    heading_ego = np.arctan2(h_ego[1], h_ego[0])
                    v_world = np.array([speed * math.cos(yaw_rh), speed * math.sin(yaw_rh), 0.0], dtype=np.float32)
                    v_ego = rot @ v_world
                    vx = v_ego[0]
                    vy = v_ego[1]
                    width = dic['extent'][1] * 2
                    length = dic['extent'][0] * 2
                    # è®¡ç®—æœªæ¥80å¸§
                    if t >= idx+1:
                        agent_features_future[rid, tt-21] = np.array([x, y, heading_ego], dtype=np.float32)
                    # è®¡ç®—è¿‡å»å’Œç°åœ¨21å¸§
                    if dic['class'] == 'vehicle' and t <= idx:
                        agent_features_past[rid, tt] = np.array([x, y, np.cos(heading_ego), np.sin(heading_ego),
                                                     vx, vy, width, length,
                                                     1.0, 0.0, 0.0], dtype=np.float32)
                    elif dic['class'] == 'walker' and t <= idx:
                        agent_features_past[rid, tt] = np.array([x, y, np.cos(heading_ego), np.sin(heading_ego),
                                                     vx, vy, width, length,
                                                     0.0, 1.0, 0.0], dtype=np.float32)
                    elif dic['class'] == 'bicycle' and t <= idx:
                        agent_features_past[rid, tt] = np.array([x, y, np.cos(heading_ego), np.sin(heading_ego),
                                                     vx, vy, width, length,
                                                     0.0, 0.0, 1.0], dtype=np.float32)
        return ego_agent_future, agent_features_past, agent_features_future
    
    # è·å–æ¯ä¸€å¸§çš„äº¤é€šç¯ä¿¡å·
    def process_tls(self, idx) -> Dict:
        tls = {}
        with gzip.open(self._log_file[idx], 'rt', encoding='utf-8') as gz_file:
            anno = json.load(gz_file)
            for dic in anno['bounding_boxes']:
                if dic['class']=='traffic_light' and dic['affects_ego']==True:
                    point=[(dic['trigger_volume_location'][0],-dic['trigger_volume_location'][1])]
                    lane_keys=get_keys_for_points_in_polygons(point,self.lane_polygons,self.lane_tree)[0]
                    for lane_key in lane_keys:
                        tls[lane_key] = self.tls_dict[dic['state']]
        return tls

    def get_static(self):
        static_objects = np.zeros((self.static_objects_num, 10), dtype=np.float32)
        return static_objects
    
    def _get_crosswalk_edges(
        self, crosswalk, sample_points: int = 21
    ):
        bbox = shapely.minimum_rotated_rectangle(crosswalk)
        coords = np.stack(bbox.exterior.coords.xy, axis=-1)
        edge1 = coords[[3, 0]]  # right boundary
        edge2 = coords[[2, 1]]  # left boundary

        edges = np.stack([(edge1 + edge2) * 0.5, edge2, edge1], axis=0)  # [3, 2, 2]
        vector = edges[:, 1] - edges[:, 0]  # [3, 2]
        steps = np.linspace(0, 1, sample_points, endpoint=True)[None, :]
        points = edges[:, 0][:, None, :] + vector[:, None, :] * steps[:, :, None]

        return points
    
    def world_points_to_ego(self, points_xy: np.ndarray, world2ego: np.ndarray) -> np.ndarray:
        """
        points_xy: (P, 2) world coords
        world2ego: (4, 4) transform
        return: (P, 2) ego coords
        """
        if points_xy.ndim != 2 or points_xy.shape[1] != 2:
            raise ValueError(f"points_xy shape should be (P, 2), got {points_xy.shape}")
        if world2ego.shape != (4, 4):
            raise ValueError(f"world2ego shape should be (4, 4), got {world2ego.shape}")

        P = points_xy.shape[0]
        ones = np.ones((P, 1), dtype=points_xy.dtype)
        zeros = np.zeros((P, 1), dtype=points_xy.dtype)
        pts_h = np.hstack([points_xy, zeros, ones])  # (P, 4)

        pts_ego_h = (world2ego @ pts_h.T).T  # (P, 4)
        return pts_ego_h[:, :2]

    # è¾“å…¥ego location, æå–å‘¨å›´åœ°å›¾ç‰¹å¾
    def get_map_features(self, idx, query_xy):
        # è·å¾—è½¬æ¢çŸ©é˜µ
        w2e = None
        with gzip.open(self._log_file[idx], 'rt', encoding='utf-8') as gz_file:
            anno = json.load(gz_file)
            for dic in anno['bounding_boxes']:
                if dic['class'] == 'ego_vehicle':
                    world2ego = dic['world2ego']
                    w2e = np.array(world2ego, dtype=np.float32)
                    w2e = self._to_right_hand_transform(w2e)
                    break
        if w2e is None:
            raise ValueError(f"ego_vehicle not found in {self._log_file[idx]}")
        # æŸ¥è¯¢egoé™„è¿‘çš„è½¦é“
        x_min, x_max = query_xy[0] - self.radius, query_xy[0] + self.radius
        y_min, y_max = query_xy[1] - self.radius, query_xy[1] + self.radius
        patch = box(x_min, y_min, x_max, y_max)
        lane_keys=fast_intersection_query_keys(self.lane_polygons,patch,self.lane_tree)
        crs_keys=fast_intersection_query_keys(self.crs_info,patch,self.crs_tree)
        P = self.lane_len # é‡‡æ ·ç‚¹æ•°
        lanes = np.zeros((self.lane_num, self.lane_len, 12), dtype=np.float32)
        lanes_speed_limit = np.zeros((self.lane_num, 1), dtype=np.float32)
        lanes_has_speed_limit = np.zeros((self.lane_num, 1), dtype=np.bool_)
        route_lanes = np.zeros((self.route_num, self.lane_len, 12), dtype=np.float32)
        route_lanes_speed_limit = np.zeros((self.route_num, 1), dtype=np.float32)
        route_lanes_has_speed_limit = np.zeros((self.route_num, 1), dtype=np.bool_)
        # äº¤é€šç¯ä¿¡æ¯
        tls = self.process_tls(idx)
        for i,lane_key in enumerate(lane_keys):
            if i >= self.lane_num:
                break
            lane=self.lane_info[lane_key]
            if np.asarray(lane[0].coords).shape==():
                print("empty_laneid:",lane_key,len(lane[0].coords))
            if lane_key in tls.keys():
                tls_one_hot = tls[lane_key]
            else:
                # å¦‚æœæ²¡è¢«è®°å½•å°±æ˜¯UnknownçŠ¶æ€
                tls_one_hot = [0, 0, 0, 1]
            tls_one_hot = np.array(tls_one_hot, dtype=np.float32) # (4,)
            tls_one_hot = np.tile(tls_one_hot, (self.lane_len, 1)) # (20, 4)
            centerline=interpolate_polyline(
                np.asarray(lane[0].coords), P + 1
            )
            left_bound=interpolate_polyline(
                np.asarray(lane[1].coords), P 
            )
            right_bound=interpolate_polyline(
                np.asarray(lane[2].coords), P
            )
            # è½¬æ¢åˆ°egoåæ ‡ç³»
            centerline = self.world_points_to_ego(centerline, w2e)
            left_bound = self.world_points_to_ego(left_bound, w2e)
            right_bound = self.world_points_to_ego(right_bound, w2e)
            lanes[i][:, :2] = centerline[:P, :]
            lanes[i][:, 2:4] = np.diff(centerline, axis=0)
            lanes[i][:, 4:6] = left_bound - centerline[:P, :]
            lanes[i][:, 6:8] = right_bound - centerline[:P, :]
            lanes[i][:, 8:12] = tls_one_hot
            lanes_has_speed_limit[i] = False
        # ç­›é€‰route_lanes
        count = 0
        for i, lane_key in enumerate(lane_keys):
            if lane_key in self.route_ids:
                route_lanes[count] = lanes[i]
                route_lanes_has_speed_limit[count] = False
                count += 1
            if count >= self.route_num:
                break
        return lanes, lanes_speed_limit, lanes_has_speed_limit, route_lanes, route_lanes_speed_limit, route_lanes_has_speed_limit
        

    def process_map(self):
        cache_path = None
        if self._cache_dir:
            # ä½¿ç”¨ map_file_path çš„åŸºç¡€å (townid_HD_map) ä½œä¸ºç¼“å­˜ID
            map_filename = os.path.basename(self.map_file_path)
            map_id = os.path.splitext(map_filename)[0] 
            full_cache_dir = os.path.join(self._cache_dir, "AAmap_cache")
            if not os.path.exists(full_cache_dir):
                os.makedirs(full_cache_dir)
            cache_path = os.path.join(full_cache_dir, f"{map_id}_processed.pkl")
        if cache_path and os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    cached_data = pickle.load(f)
                self.lane_info = cached_data['lane_info']
                self.next_lane = cached_data['next_lane']
                self.crs_info = cached_data['crs_info']
                logger.info(f"âœ… Map data loaded from cache: {cache_path}")
                return # ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥è¿”å›
            except Exception as e:
                logger.warning(f"Failed to load map cache from {cache_path}: {e}. Recalculating map features.")
        # logger.info(f"â³ Processing map file (no cache): {self.map_file_path}")
        # æ‰§è¡ŒåŸå§‹åœ°å›¾å¤„ç†é€»è¾‘ï¼Œå¹¶å°†ç»“æœä¿å­˜åœ¨ self å±æ€§ä¸­
        self.process_map_logic() 
        if cache_path:
            try:
                data_to_cache = {
                    'lane_info': self.lane_info,
                    'next_lane': self.next_lane,
                    'crs_info': self.crs_info,
                }
                with open(cache_path, 'wb') as f:
                    pickle.dump(data_to_cache, f)
                logger.info(f"ğŸ’¾ Map data successfully cached to {cache_path}")
            except Exception as e:
                logger.error(f"Failed to save map cache for {self.map_file_path}: {e}")
        
    
    def process_map_logic(self):
        dic1={}
        with np.load(self.map_file_path, allow_pickle=True) as data:
            arr_data = data['arr']
            for item in arr_data:
                for lane_id,volume in item[1].items():
                    ##lane
                    road_id=item[0]
                    # print(road_id)
                    if lane_id!='Trigger_Volumes':
                        points_c=[]
                        points_b=[]
                        self.next_lane[(item[0],lane_id)]=[]
                        for single_lane in volume: # æ¯ä¸€æ®µ
                            points=[]
                            for point in single_lane['Points']: # point = ((x, y, z), (roll, pitch, yaw), flag)
                                points.append((point[0][0],-point[0][1]))
                            # if len(points)>50000:points=points[::10]
                            sc=len(points)//500
                            if sc > 1:
                                last_point = points[-1]  # è®°å½•åŸç‚¹é›†æœ€åä¸€ä¸ªç‚¹
                                points = points[::sc]    # æ­¥é•¿é‡‡æ ·
                                if points[-1] != last_point: 
                                    points.append(last_point) # è¡¥ä¸Šæœ€åä¸€ä¸ªç‚¹
                            if single_lane['Type']=='Center':points_c.append(points)
                            else:points_b.append(points)
                            if 'Topology' in single_lane:
                                for nxt_id in single_lane['Topology']:
                                    if nxt_id!=(item[0],lane_id) and nxt_id not in self.next_lane[(item[0],lane_id)]:
                                        self.next_lane[(item[0],lane_id)].append(nxt_id)
                        points_c=filter_subsets(points_c)
                        if len(points_c)==0:continue
                        center_ls=merge_segments_by_shortest_distance(points_c)
                        classfi=classify_multiple_boundary_segments(center_ls,points_b)
                        points_l=[] # å·¦ä¾§è¾¹ç•Œçº¿
                        points_r=[] # å³ä¾§è¾¹ç•Œçº¿
                        for points,label in zip(points_b,classfi):
                            if label=='Left':points_l.append(points)
                            elif label=='Right':points_r.append(points)
                        points_l=filter_subsets(points_l)
                        points_r=filter_subsets(points_r)
                        # if item[0]==539 and lane_id==-1:
                        #     for point in points_l:
                        #         print(point[0],point[-1])
                        #     for point in points_r:
                        #         print(point[0],point[-1])
                        if len(points_l)==0:print("l",item[0],lane_id,len(points_l),len(points_r),len(points_b))   
                        if len(points_r)==0:print("r",item[0],lane_id,len(points_r))   
                        lbound_ls=merge_segments_by_shortest_distance(points_l)
                        rbound_ls=merge_segments_by_shortest_distance(points_r)
                        lbound_ls,rbound_ls=align_laneline_order(lbound_ls,rbound_ls)
                        self.lane_info[(road_id,lane_id)]=[center_ls,lbound_ls,rbound_ls]#è¿™æ ·å­˜å‚¨å¥½å—ï¼Œæœ‰æ›´å¿«çš„æ–¹æ³•å—
                        dic1[(item[0],lane_id)]=create_lane_polygon(lbound_ls,rbound_ls)
                    #crosswalk
                    else:
                        for sec_id,single_lane in enumerate(volume):
                            if single_lane['Type']!='TrafficLight':
                                crs_points=[(point[0],-point[1]) for point in single_lane['Points']]
                                self.crs_info[(road_id,lane_id,sec_id)]=Polygon(crs_points)
        # self.route_ids=get_lane_ids_from_trajectory(dic1,self.next_lane,self.trajectory)

